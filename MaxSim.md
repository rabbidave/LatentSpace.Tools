
# MaxSim: Per-Token Relevance Calculation


---


### 1. Introduction

The **MaxSim (Maximum Similarity)** algorithm provides a fine-grained method for calculating the relevance score between a Query and a Document. Unlike approaches that use single summary vectors, MaxSim operates at the token level, evaluating how well each individual query token is semantically matched by any token within the document. This score is crucial for re-ranking search results or as a core part of dense retrieval models.

### 2. Core Principle: The "Searchlight" Analogy

Imagine the Query as a set of "searchlights." Each searchlight (representing a query token) independently scans the entire "landscape" (the Document) to find the single point where its semantic meaning is most strongly reflected or "illuminated." The overall relevance is then the sum of the brightness from these individual best illuminations.

### 3. System Inputs

The MaxSim process takes two primary inputs:

*   **Query (Q):** A string representing the user's information need.
    *   *Example:* `Q = "AI ethics"`
*   **Document (D):** A string representing a document from the corpus to be scored against the query.
    *   *Example:* `D = "Debate on AI governance and the ethics of artificial intelligence."`

### 4. Component 1: Tokenization & Semantic Embedding

Before comparison, both Query and Document undergo tokenization and are transformed into sequences of contextual embeddings.

**4.1. Tokenization:**
The input strings are broken down into individual tokens. The specifics depend on the tokenizer used (e.g., WordPiece, SentencePiece, BPE).

*   **Query Tokens (Q<sub>t</sub>):**
    *   `Q = "AI ethics"`  ->  `Q_t1 [AI]`, `Q_t2 [ethics]`
*   **Document Tokens (D<sub>t</sub>):**
    *   `D = "Debate on AI governance and the ethics of artificial intelligence."`
    *   -> `D_t1 [Debate]`, `D_t2 [on]`, `D_t3 [AI]`, `D_t4 [governance]`, `D_t5 [and]`, `D_t6 [the]`, `D_t7 [ethics]`, `D_t8 [of]`, `D_t9 [artificial]`, `D_t10 [intelligence]`

    > **Note on Tokenization:** Tokenizers can sometimes split words in unexpected ways (e.g., "Debate" might become `["Deb", "ate"]` or stay as `["Debate"]`). The quality of the downstream embeddings often handles these variations, but it's a factor in the pre-processing pipeline. For this example, we assume `[Debate]` is a single token.

**4.2. Semantic Embedding:**
Each token is converted into a high-dimensional dense vector (embedding) that captures its contextual meaning. These are typically generated by pre-trained transformer models like BERT, DistilBERT, or specialized ColBERT encoders.

*   `emb(Q_t1)`: Embedding vector for "AI" (in context of query).
*   `emb(D_t3)`: Embedding vector for "AI" (in context of document).

---

---



---

### 5. Component 2: MaxSim Interaction - Per-Query-Token Similarity Search

This is the core of MaxSim. For each query token embedding, we calculate its cosine similarity against *every* document token embedding. From these scores, we select the *maximum*.

**5.1. Interaction for Query Token Q<sub>t1</sub> [AI]**

The "searchlight" for `Q_t1 [AI]` scans all document tokens. We calculate `cosine_similarity(emb(Q_t1), emb(D_ti))` for `i=1 to 10`.

*Conceptual Similarity Matrix Row for Q<sub>t1</sub> [AI]:*
```ascii
Comparing Q₁[AI] with Doc Token...| D₁  | D₂ | D₃   | D₄  | D₅ | D₆ | D₇   | D₈ | D₉    | D₁₀    |
(Doc Token Hint)                  | Deb | on | AI   | gov |and |the | eth  | of | artif | intel  |
----------------------------------|-----|----|------|-----|----|----|------|----|-------|--------|
Q₁ Similarity Score:              | 0.1 |0.0 |*0.95*| 0.3 |0.0 |0.0 | 0.2  |0.0 | 0.8   | *0.85* |
```

*   **Maximum Similarity for Q<sub>t1</sub>:**
    `MaxSim(Q_t1) = max(0.1, 0.0, 0.95, 0.3, 0.0, 0.0, 0.2, 0.0, 0.8, 0.85) = 0.95`
    This maximum score is achieved with `D_t3 [AI]`.
    *(Note: D<sub>t9</sub> [artificial] and D<sub>t10</sub> [intelligence] also show strong semantic similarity to "AI", highlighting that MaxSim captures meaning beyond exact matches. If D<sub>t3</sub> was absent, one of these would likely become the max for Q<sub>t1</sub>.)*

**5.2. Interaction for Query Token Q<sub>t2</sub> [ethics]**

The "searchlight" for `Q_t2 [ethics]` scans all document tokens. We calculate `cosine_similarity(emb(Q_t2), emb(D_ti))` for `i=1 to 10`.

*Conceptual Similarity Matrix Row for Q<sub>t2</sub> [ethics]:*
```ascii
Comparing Q₂[ethics] with Doc Token..| D₁  | D₂ | D₃  | D₄  | D₅ | D₆ | D₇    | D₈ | D₉    | D₁₀   |
(Doc Token Hint)                     | Deb | on | AI  | gov |and |the | eth   | of | artif | intel |
-------------------------------------|-----|----|-----|-----|----|----|-------|----|-------|-------|
Q₂ Similarity Score:                 | 0.2 |0.0 | 0.1 | 0.6 |0.0 |0.0 |*0.92* |0.0 | 0.05  | 0.1   |
```

*   **Maximum Similarity for Q<sub>t2</sub>:**
    `MaxSim(Q_t2) = max(0.2, 0.0, 0.1, 0.6, 0.0, 0.0, 0.92, 0.0, 0.05, 0.1) = 0.92`
    This maximum score is achieved with `D_t7 [ethics]`.

### 6. Component 3: Aggregation

The individual maximum similarity scores obtained for each query token are aggregated (typically summed) to produce the final MaxSim score for the Query-Document pair.

*   **Collected Maximum Scores:**
    *   `MaxSim(Q_t1) = 0.95`
    *   `MaxSim(Q_t2) = 0.92`

*   **Final MaxSim Score (Q, D):**
    `Score(Q,D) = Sum( MaxSim(Q_ti) for each query token i )`
    `Score(Q,D) = 0.95 + 0.92 = 1.87`

### 7. System Output

*   **Relevance Score:** A single floating-point number (e.g., `1.87`) representing the MaxSim relevance between the Query and the Document. Higher scores indicate greater relevance.

### 8. Key Architectural Takeaways

*   **Late Interaction:** Semantic embeddings for query and document tokens are generated first; detailed comparison (MaxSim) happens *after* encoding.
*   **Granular Evidence Accumulation:** The final score reflects the sum of the strongest individual semantic matches for each query term. This allows documents to score well if they address multiple query aspects, even if those aspects are located in different parts of the document.
*   **Robustness to Wording:** Reliance on semantic similarity of contextual embeddings (rather than exact keyword matching) makes the system more robust to variations in phrasing.
*   **Computational Profile:** Involves `N_q * N_d` similarity calculations per query-document pair (where `N_q` is number of query tokens, `N_d` is number of document tokens), followed by `N_q` max operations and a sum. Optimized implementations exist (e.g., using batch matrix multiplications in ColBERT).

---
